import{s as Le,b as Ut,e as ve,n as Te}from"../chunks/scheduler.CufrTsdM.js";import{S as Me,i as ke,c as Dt,a as St,m as Bt,t as Rt,e as Gt,g as Nt,h as n,s as r,n as G,H as tt,j as i,k as p,b as o,o as x,p as N,q as et,f as e,l as L,d as a,r as h}from"../chunks/index.CufBhhP2.js";import{g as He,a as ge}from"../chunks/spread.CgU5AtxT.js";import{B as Pe}from"../chunks/blog.BwWZGTwR.js";import{R as $e}from"../chunks/resources.Bt5hxEn2.js";import{C as ze}from"../chunks/cite.Cg45sgJ2.js";function Ae(j){let l,w="Michael T. Pearce, <b>Thomas Dooms</b>, Alice Rigg",d,c,m="",u,v,st,T,jt='<p>This page has been superseded in favour of our new <a href="https://arxiv.org/pdf/2410.08417" rel="nofollow">paper</a>. The new page can be found <a href="/research/bilinear">here</a>.</p>',at,M,Ot="Background",nt,k,Vt=`Understanding models from their weights has long been a pipedream of many interpretability researchers.
The main obstacle to this is that ReLUs (or non-linear activation functions in general) are not meaningful without inputs.
Since, any small change in input can cause a ReLU to “activate”.
The <em>only</em> way to compute the output of a ReLU is to actually provide it an input and just see what happens.
This has made it notoriously hard to put any form of guarantees on model behaviour and has led to a plethora of input-dependent research in the past.
The one thing giving rise to the astounding capabilities is also the thing that makes them <em>really</em> hard to study.`,it,H,Ft=`Bilinear layers, may be able to offer us the best of both worlds; weights which we can meaningfully study while retaining the current capabilities.
Bilinear layers are part of the gated linear unit (GLU) family, which are gaining lots of traction due to their accuracy benefits recently.
The bilinear layer is the simplest form of a GLU, using no activation function at all.
One prominent question that is often asked at this point is “if this is just doing linear operations, how can this be a good model component”?
While either side <em>is</em> linear, the whole is bilinear, which is non-linear to the input, which is all we need.`,lt,y,Jt=`<div class="column is-narrow"><figure><img src="/research/bilinear/glu.svg" alt="GLU &amp; Bilinear"/> <figcaption>An ordinary GLU (left) and a bilinear layer (right). An ordinary GLU has a gate part which &quot;selects&quot; <br/> which parts of the other side should be kept.
        In the case of a bilinear layer, this selection is instead continuous.</figcaption></figure></div>`,rt,P,Kt="As foreshadowed, the linearity of each branch is a really useful for interpretability. It allows to use techniques from linear algebra such as SVD and have them actually be meaningful. However, that’s not all, each output of a bilinear layer can be described quite elegantly; it’s a sum of (weighted) pairwise input feature interactions. Note that this is impossible to do for a layer with a ReLU (or other activation functions), there is no clean formula that describes how features interact.",ot,$,Qt="Decomposing The Weights",pt,z,Xt=`We can exploit all these facts to find useful direction for each output of such a layer.
While we won’t go into details, these are the main ideaas behind the decomposition.`,mt,g,Y,b,Ht,O,ht,we='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="bold">x</mtext><mi>A</mi><mtext mathvariant="bold">x</mtext></mrow><annotation encoding="application/x-tex">\\textbf{x}A\\textbf{x}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord text"><span class="mord textbf">x</span></span><span class="mord mathnormal">A</span><span class="mord text"><span class="mord textbf">x</span></span></span></span></span>',Pt,V,ct,be='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">A</span></span></span></span>',$t,zt,Z,f,At,F,ut,ye='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mo>∑</mo><mi>i</mi></msub><msub><mi>λ</mi><mi>i</mi></msub><msub><mi>v</mi><mi>i</mi></msub><mo>⊗</mo><msub><mi>v</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\\sum_i \\lambda_i v_i \\otimes v_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0497em;vertical-align:-0.2997em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.162em;"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2997em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">λ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⊗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>',qt,J,ft,_e='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>⊗</mo></mrow><annotation encoding="application/x-tex">\\otimes</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord">⊗</span></span></span></span>',Et,K,dt,Ce='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">λ</span></span></span></span>',Wt,It,Q,Yt="<p>Since a sum of symmetric matrices is still symmetric, this can be done for any output direction not just single outputs. For instance, we can take the difference between two features by subtracting the interaction matrices.</p>",vt,A,Zt="The following images depict the most important eigenvector for a set of digits in a single-layer MNIST model.",gt,_,te='<div class="column"><img src="/research/bilinear/digit_0.svg" alt="digit 0"/></div> <div class="column"><img src="/research/bilinear/digit_1.svg" alt="digit 1"/></div> <div class="column"><img src="/research/bilinear/digit_5.svg" alt="digit 5"/></div> <div class="column"><img src="/research/bilinear/digit_6.svg" alt="digit 6"/></div>',xt,q,ee="This shows that exploiting these properties of the bilinear layer can yield very interpretable structure from the weights alone!",wt,E,se="Language Models",bt,W,ae=`We can use the same technique of replacing MLPs in a transformer with its bilinear variant.
This allows us to understand the computation going on in MLPs on a deep level.`,yt,I,ne=`The following figure specifically extends the technique to attention, we derive the most important features for swim and then examine which samples activate them most strongly.
We see that the first feature fires on sea related contexts while the second is mostly grammatical.`,_t,U,ie='<img src="/research/bilinear/swim-cropped.svg" alt="Eigenvectors of swim" width="880"/> <figcaption></figcaption>',Ct,D,le="Future Work",Lt,S,re=`Currently, this approach is only feasible on shallow models as the number of eigenvectors grows exponentially with layer size.
We are exploring techniques to reduce this.`,Tt,B,oe=`This work mostly aimed to show the interpretability instead of leveraging the decomposition towards concrete findings.
One could imagine using this to uncover induction behaviour in language models or curve detector circuits in vision models in a principled manner.`,Mt,R,kt;return v=new $e({props:{paper:"https://arxiv.org/abs/2406.03947",code:"https://github.com/tdooms/bilinear-decomposition",models:"https://huggingface.co/collections/tdooms/bilinear-66991d82406db9529a7170d2"}}),R=new ze({props:{text:`@misc{pearce2024weightbaseddecompositioncasebilinear,
        title={Weight-based Decomposition: A Case for Bilinear MLPs},
        author={Michael T. Pearce and Thomas Dooms and Alice Rigg},
        year={2024},
        eprint={2406.03947},
        archivePrefix={arXiv},
        primaryClass={cs.LG},
        url={https://arxiv.org/abs/2406.03947},
    }`}}),{c(){l=n("p"),l.innerHTML=w,d=r(),c=n("div"),c.innerHTML=m,u=r(),Dt(v.$$.fragment),st=r(),T=n("blockquote"),T.innerHTML=jt,at=r(),M=n("h3"),M.textContent=Ot,nt=r(),k=n("p"),k.innerHTML=Vt,it=r(),H=n("p"),H.innerHTML=Ft,lt=r(),y=n("div"),y.innerHTML=Jt,rt=r(),P=n("p"),P.textContent=Kt,ot=r(),$=n("h3"),$.textContent=Qt,pt=r(),z=n("p"),z.textContent=Xt,mt=r(),g=n("ol"),Y=n("li"),b=n("p"),Ht=G("The feature interactions of a bilinear layer output can be written as "),O=n("span"),ht=new tt(!1),Pt=G(" where we call "),V=n("span"),ct=new tt(!1),$t=G(" the interaction matrix. Each entry in the matrix is the weight of how strong the feature at the row and column should interact. Due to this structure, it is a symmetric matrix. While this is nice, this interaction matrix is hard to visualize for any non-trivial problem (especially if the features are structured, such as images)."),zt=r(),Z=n("li"),f=n("p"),At=G("Luckily, we can leverage this symmetric property and perform an eigendecomposition on these matrices. This operation decomposes the matrix into "),F=n("span"),ut=new tt(!1),qt=G(" (where "),J=n("span"),ft=new tt(!1),Et=G(" is an outer product) where values of "),K=n("span"),dt=new tt(!1),Wt=G(" decrease. In words, this finds which vectors best describe the interaction matrix. We can then easily visualize these vectors."),It=r(),Q=n("li"),Q.innerHTML=Yt,vt=r(),A=n("p"),A.textContent=Zt,gt=r(),_=n("div"),_.innerHTML=te,xt=r(),q=n("p"),q.textContent=ee,wt=r(),E=n("h3"),E.textContent=se,bt=r(),W=n("p"),W.textContent=ae,yt=r(),I=n("p"),I.textContent=ne,_t=r(),U=n("figure"),U.innerHTML=ie,Ct=r(),D=n("h3"),D.textContent=le,Lt=r(),S=n("p"),S.textContent=re,Tt=r(),B=n("p"),B.textContent=oe,Mt=r(),Dt(R.$$.fragment),this.h()},l(t){l=i(t,"P",{"data-svelte-h":!0}),p(l)!=="svelte-1p7kwfh"&&(l.innerHTML=w),d=o(t),c=i(t,"DIV",{class:!0,"data-svelte-h":!0}),p(c)!=="svelte-wdfy0z"&&(c.innerHTML=m),u=o(t),St(v.$$.fragment,t),st=o(t),T=i(t,"BLOCKQUOTE",{"data-svelte-h":!0}),p(T)!=="svelte-1xt9cz3"&&(T.innerHTML=jt),at=o(t),M=i(t,"H3",{"data-svelte-h":!0}),p(M)!=="svelte-1q8ftns"&&(M.textContent=Ot),nt=o(t),k=i(t,"P",{"data-svelte-h":!0}),p(k)!=="svelte-tzl0b3"&&(k.innerHTML=Vt),it=o(t),H=i(t,"P",{"data-svelte-h":!0}),p(H)!=="svelte-2f7l7x"&&(H.innerHTML=Ft),lt=o(t),y=i(t,"DIV",{class:!0,"data-svelte-h":!0}),p(y)!=="svelte-1ks8fi9"&&(y.innerHTML=Jt),rt=o(t),P=i(t,"P",{"data-svelte-h":!0}),p(P)!=="svelte-n9pg3x"&&(P.textContent=Kt),ot=o(t),$=i(t,"H3",{"data-svelte-h":!0}),p($)!=="svelte-15y9o0a"&&($.textContent=Qt),pt=o(t),z=i(t,"P",{"data-svelte-h":!0}),p(z)!=="svelte-3ulqu2"&&(z.textContent=Xt),mt=o(t),g=i(t,"OL",{});var s=x(g);Y=i(s,"LI",{});var pe=x(Y);b=i(pe,"P",{});var X=x(b);Ht=N(X,"The feature interactions of a bilinear layer output can be written as "),O=i(X,"SPAN",{class:!0});var me=x(O);ht=et(me,!1),me.forEach(e),Pt=N(X," where we call "),V=i(X,"SPAN",{class:!0});var he=x(V);ct=et(he,!1),he.forEach(e),$t=N(X," the interaction matrix. Each entry in the matrix is the weight of how strong the feature at the row and column should interact. Due to this structure, it is a symmetric matrix. While this is nice, this interaction matrix is hard to visualize for any non-trivial problem (especially if the features are structured, such as images)."),X.forEach(e),pe.forEach(e),zt=o(s),Z=i(s,"LI",{});var ce=x(Z);f=i(ce,"P",{});var C=x(f);At=N(C,"Luckily, we can leverage this symmetric property and perform an eigendecomposition on these matrices. This operation decomposes the matrix into "),F=i(C,"SPAN",{class:!0});var ue=x(F);ut=et(ue,!1),ue.forEach(e),qt=N(C," (where "),J=i(C,"SPAN",{class:!0});var fe=x(J);ft=et(fe,!1),fe.forEach(e),Et=N(C," is an outer product) where values of "),K=i(C,"SPAN",{class:!0});var de=x(K);dt=et(de,!1),de.forEach(e),Wt=N(C," decrease. In words, this finds which vectors best describe the interaction matrix. We can then easily visualize these vectors."),C.forEach(e),ce.forEach(e),It=o(s),Q=i(s,"LI",{"data-svelte-h":!0}),p(Q)!=="svelte-1xlpf8v"&&(Q.innerHTML=Yt),s.forEach(e),vt=o(t),A=i(t,"P",{"data-svelte-h":!0}),p(A)!=="svelte-qh21fd"&&(A.textContent=Zt),gt=o(t),_=i(t,"DIV",{class:!0,"data-svelte-h":!0}),p(_)!=="svelte-zanb88"&&(_.innerHTML=te),xt=o(t),q=i(t,"P",{"data-svelte-h":!0}),p(q)!=="svelte-1nvlwio"&&(q.textContent=ee),wt=o(t),E=i(t,"H3",{"data-svelte-h":!0}),p(E)!=="svelte-53090i"&&(E.textContent=se),bt=o(t),W=i(t,"P",{"data-svelte-h":!0}),p(W)!=="svelte-1fiaazj"&&(W.textContent=ae),yt=o(t),I=i(t,"P",{"data-svelte-h":!0}),p(I)!=="svelte-o6ph00"&&(I.textContent=ne),_t=o(t),U=i(t,"FIGURE",{"data-svelte-h":!0}),p(U)!=="svelte-pftcks"&&(U.innerHTML=ie),Ct=o(t),D=i(t,"H3",{"data-svelte-h":!0}),p(D)!=="svelte-1r68seq"&&(D.textContent=le),Lt=o(t),S=i(t,"P",{"data-svelte-h":!0}),p(S)!=="svelte-1ieuwrf"&&(S.textContent=re),Tt=o(t),B=i(t,"P",{"data-svelte-h":!0}),p(B)!=="svelte-k328po"&&(B.textContent=oe),Mt=o(t),St(R.$$.fragment,t),this.h()},h(){L(c,"class","mt-6"),L(y,"class","columns is-centered"),ht.a=null,L(O,"class","math math-inline"),ct.a=null,L(V,"class","math math-inline"),ut.a=null,L(F,"class","math math-inline"),ft.a=null,L(J,"class","math math-inline"),dt.a=null,L(K,"class","math math-inline"),L(_,"class","columns")},m(t,s){a(t,l,s),a(t,d,s),a(t,c,s),a(t,u,s),Bt(v,t,s),a(t,st,s),a(t,T,s),a(t,at,s),a(t,M,s),a(t,nt,s),a(t,k,s),a(t,it,s),a(t,H,s),a(t,lt,s),a(t,y,s),a(t,rt,s),a(t,P,s),a(t,ot,s),a(t,$,s),a(t,pt,s),a(t,z,s),a(t,mt,s),a(t,g,s),h(g,Y),h(Y,b),h(b,Ht),h(b,O),ht.m(we,O),h(b,Pt),h(b,V),ct.m(be,V),h(b,$t),h(g,zt),h(g,Z),h(Z,f),h(f,At),h(f,F),ut.m(ye,F),h(f,qt),h(f,J),ft.m(_e,J),h(f,Et),h(f,K),dt.m(Ce,K),h(f,Wt),h(g,It),h(g,Q),a(t,vt,s),a(t,A,s),a(t,gt,s),a(t,_,s),a(t,xt,s),a(t,q,s),a(t,wt,s),a(t,E,s),a(t,bt,s),a(t,W,s),a(t,yt,s),a(t,I,s),a(t,_t,s),a(t,U,s),a(t,Ct,s),a(t,D,s),a(t,Lt,s),a(t,S,s),a(t,Tt,s),a(t,B,s),a(t,Mt,s),Bt(R,t,s),kt=!0},p:Te,i(t){kt||(Rt(v.$$.fragment,t),Rt(R.$$.fragment,t),kt=!0)},o(t){Gt(v.$$.fragment,t),Gt(R.$$.fragment,t),kt=!1},d(t){t&&(e(l),e(d),e(c),e(u),e(st),e(T),e(at),e(M),e(nt),e(k),e(it),e(H),e(lt),e(y),e(rt),e(P),e(ot),e($),e(pt),e(z),e(mt),e(g),e(vt),e(A),e(gt),e(_),e(xt),e(q),e(wt),e(E),e(bt),e(W),e(yt),e(I),e(_t),e(U),e(Ct),e(D),e(Lt),e(S),e(Tt),e(B),e(Mt)),Nt(v,t),Nt(R,t)}}}function qe(j){let l,w;const d=[j[0],xe];let c={$$slots:{default:[Ae]},$$scope:{ctx:j}};for(let m=0;m<d.length;m+=1)c=Ut(c,d[m]);return l=new Pe({props:c}),{c(){Dt(l.$$.fragment)},l(m){St(l.$$.fragment,m)},m(m,u){Bt(l,m,u),w=!0},p(m,[u]){const v=u&1?He(d,[u&1&&ge(m[0]),u&0&&ge(xe)]):{};u&2&&(v.$$scope={dirty:u,ctx:m}),l.$set(v)},i(m){w||(Rt(l.$$.fragment,m),w=!0)},o(m){Gt(l.$$.fragment,m),w=!1},d(m){Nt(l,m)}}}const xe={title:"Bilinear Decomposition",date:"18 Jul 2024",kind:"research"};function Ee(j,l,w){return j.$$set=d=>{w(0,l=Ut(Ut({},l),ve(d)))},l=ve(l),[l]}class Re extends Me{constructor(l){super(),ke(this,l,Ee,qe,Le,{})}}export{Re as component};
