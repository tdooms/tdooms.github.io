<!DOCTYPE html><html lang="en" data-theme="emerald" data-astro-cid-sckkx6r4> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width"><link rel="icon" type="image/svg+xml" href="/favicon.svg"><link rel="preconnect" href="https://fonts.googleapis.com"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link href="https://fonts.googleapis.com/css2?family=Fira+Sans:wght@300;400;500;600;700&family=Fira+Code:wght@400;500&display=swap" rel="stylesheet"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/7.0.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous"><meta name="generator" content="Astro v5.15.3"><title>tdooms</title><!-- Google tag (gtag.js) --><script async src="https://www.googletagmanager.com/gtag/js?id=G-7H9E0Y121Q"></script><script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'G-7H9E0Y121Q');
		</script><link rel="stylesheet" href="/_astro/bae.CWsAgS2g.css"></head> <body data-astro-cid-sckkx6r4> <main class="max-w-[1000px] mx-auto px-4 pt-3" data-astro-cid-sckkx6r4> <style>astro-island,astro-slot,astro-static-slot{display:contents}</style><script>(()=>{var e=async t=>{await(await t())()};(self.Astro||(self.Astro={})).load=e;window.dispatchEvent(new Event("astro:load"));})();</script><script>(()=>{var A=Object.defineProperty;var g=(i,o,a)=>o in i?A(i,o,{enumerable:!0,configurable:!0,writable:!0,value:a}):i[o]=a;var d=(i,o,a)=>g(i,typeof o!="symbol"?o+"":o,a);{let i={0:t=>m(t),1:t=>a(t),2:t=>new RegExp(t),3:t=>new Date(t),4:t=>new Map(a(t)),5:t=>new Set(a(t)),6:t=>BigInt(t),7:t=>new URL(t),8:t=>new Uint8Array(t),9:t=>new Uint16Array(t),10:t=>new Uint32Array(t),11:t=>1/0*t},o=t=>{let[l,e]=t;return l in i?i[l](e):void 0},a=t=>t.map(o),m=t=>typeof t!="object"||t===null?t:Object.fromEntries(Object.entries(t).map(([l,e])=>[l,o(e)]));class y extends HTMLElement{constructor(){super(...arguments);d(this,"Component");d(this,"hydrator");d(this,"hydrate",async()=>{var b;if(!this.hydrator||!this.isConnected)return;let e=(b=this.parentElement)==null?void 0:b.closest("astro-island[ssr]");if(e){e.addEventListener("astro:hydrate",this.hydrate,{once:!0});return}let c=this.querySelectorAll("astro-slot"),n={},h=this.querySelectorAll("template[data-astro-template]");for(let r of h){let s=r.closest(this.tagName);s!=null&&s.isSameNode(this)&&(n[r.getAttribute("data-astro-template")||"default"]=r.innerHTML,r.remove())}for(let r of c){let s=r.closest(this.tagName);s!=null&&s.isSameNode(this)&&(n[r.getAttribute("name")||"default"]=r.innerHTML)}let p;try{p=this.hasAttribute("props")?m(JSON.parse(this.getAttribute("props"))):{}}catch(r){let s=this.getAttribute("component-url")||"<unknown>",v=this.getAttribute("component-export");throw v&&(s+=` (export ${v})`),console.error(`[hydrate] Error parsing props for component ${s}`,this.getAttribute("props"),r),r}let u;await this.hydrator(this)(this.Component,p,n,{client:this.getAttribute("client")}),this.removeAttribute("ssr"),this.dispatchEvent(new CustomEvent("astro:hydrate"))});d(this,"unmount",()=>{this.isConnected||this.dispatchEvent(new CustomEvent("astro:unmount"))})}disconnectedCallback(){document.removeEventListener("astro:after-swap",this.unmount),document.addEventListener("astro:after-swap",this.unmount,{once:!0})}connectedCallback(){if(!this.hasAttribute("await-children")||document.readyState==="interactive"||document.readyState==="complete")this.childrenConnectedCallback();else{let e=()=>{document.removeEventListener("DOMContentLoaded",e),c.disconnect(),this.childrenConnectedCallback()},c=new MutationObserver(()=>{var n;((n=this.lastChild)==null?void 0:n.nodeType)===Node.COMMENT_NODE&&this.lastChild.nodeValue==="astro:end"&&(this.lastChild.remove(),e())});c.observe(this,{childList:!0}),document.addEventListener("DOMContentLoaded",e)}}async childrenConnectedCallback(){let e=this.getAttribute("before-hydration-url");e&&await import(e),this.start()}async start(){let e=JSON.parse(this.getAttribute("opts")),c=this.getAttribute("client");if(Astro[c]===void 0){window.addEventListener(`astro:${c}`,()=>this.start(),{once:!0});return}try{await Astro[c](async()=>{let n=this.getAttribute("renderer-url"),[h,{default:p}]=await Promise.all([import(this.getAttribute("component-url")),n?import(n):()=>()=>{}]),u=this.getAttribute("component-export")||"default";if(!u.includes("."))this.Component=h[u];else{this.Component=h;for(let f of u.split("."))this.Component=this.Component[f]}return this.hydrator=p,this.hydrate},e,this)}catch(n){console.error(`[astro-island] Error hydrating ${this.getAttribute("component-url")}`,n)}}attributeChangedCallback(){this.hydrate()}}d(y,"observedAttributes",["props"]),customElements.get("astro-island")||customElements.define("astro-island",y)}})();</script><astro-island uid="t5hH3" component-url="/_astro/Navbar.CzbVORO_.js" component-export="default" renderer-url="/_astro/client.svelte.d1eApNjM.js" props="{&quot;data-astro-cid-sckkx6r4&quot;:[0,true]}" ssr client="load" opts="{&quot;name&quot;:&quot;Navbar&quot;,&quot;value&quot;:true}" await-children><!--[--><div class="bg-base-100 border-b border-base-300"><div class="container mx-auto w-full px-4 pt-4"><div class="flex items-baseline justify-center w-full"><div role="tablist" class="tabs tabs-lift tabs-lg -mb-px"><!--[--><a href="/research" role="tab" class="tab  svelte-d8j1hi"><i class="fas fa-atom mr-2 svelte-d8j1hi"></i> Research</a><a href="/demos" role="tab" class="tab  svelte-d8j1hi"><i class="fas fa-flask mr-2 svelte-d8j1hi"></i> Demos</a><!--]--> <a href="/" class="px-6 text-3xl no-underline hover:scale-105 transition-transform font-bold">tdooms</a> <!--[--><a href="/blog" role="tab" class="tab  svelte-d8j1hi"><i class="fas fa-fire mr-2 svelte-d8j1hi"></i> Blog</a><a href="/resume" role="tab" class="tab  svelte-d8j1hi"><i class="fas fa-file-alt mr-2 svelte-d8j1hi"></i> Resume</a><!--]--></div></div></div></div><!--]--><!--astro:end--></astro-island> <div class="my-8" data-astro-cid-sckkx6r4></div>  <h1 class="text-4xl font-bold mb-2">Weight-based interpretability</h1> <p>  Michael T. Pearce* ,  <strong>Thomas Dooms*</strong> ,  Alice Rigg ,  Jose M. Oramas ,  Lee Sharkey  </p> <div class="mt-6"></div> <div class="flex flex-wrap justify-center gap-4 max-w-3xl mx-auto"> <a href="https://arxiv.org/pdf/2410.08417" target="_blank" class="card bg-white shadow-md hover:shadow-lg transition-shadow p-6 text-center block w-32"> <span class="text-4xl block mb-2"> <i class="fa-regular fa-file"></i> </span> <p class="text-gray-900">Paper</p> </a><a href="https://github.com/tdooms/bilinear-decomposition" target="_blank" class="card bg-white shadow-md hover:shadow-lg transition-shadow p-6 text-center block w-32"> <span class="text-4xl block mb-2"> <i class="fa-solid fa-code"></i> </span> <p class="text-gray-900">Code</p> </a><a href="https://huggingface.co/collections/tdooms/bilinear-transformers-tinystories-670e352e0f552bab121d861f" target="_blank" class="card bg-white shadow-md hover:shadow-lg transition-shadow p-6 text-center block w-32"> <span class="text-4xl block mb-2"> <i class="fa-solid fa-database"></i> </span> <p class="text-gray-900">Models</p> </a><a href="/demos/eigenvectors" class="card bg-white shadow-md hover:shadow-lg transition-shadow p-6 text-center block w-32"> <span class="text-4xl block mb-2"> <i class="fa-solid fa-flask"></i> </span> <p class="text-gray-900">Demo</p> </a> </div> <div class="prose prose-lg max-w-none mt-6"> <h3 id="background">Background</h3>
<p>Understanding models from their weights has long been a pipedream of many interpretability researchers.
The main obstacle to this is that ReLUs (or non-linear activation functions in general) obscure the relation between inputs,
outputs, and weights. Since any slight change in input can cause a ReLU to activate. The only way to compute a ReLU’s output
is to provide an input and see what happens. Consequently, input features can interact in convoluted ways,
making it notoriously hard to guarantee model behavior, and this has led to a plethora of input-dependent research in the past.
The one thing that gives rise to astounding capabilities is also what makes them hard to study.</p>
<p>Bilinear layers are a promising alternative to ordinary layers, offering the best of both worlds: high accuracy and meaningful weights.
They are part of the gated linear unit (GLU) family, which has recently gained traction due to their accuracy benefits.
The bilinear layer is the simplest form of a GLU, using no activation function at all. Put differently, a bilinear layer replaces the activation function with a learnable matrix that operates in parallel, not sequence.
One prominent question is: “If this is just doing linear operations, how can it be a good model component?”.
While either side is linear, the whole is bilinear, which is non-linear to the input, which is all we need.</p>
<figure class="text-center"><img src="/_astro/comparison.B1-iLR8X_1SS6Dw.svg" alt="Three common MLPs: A ReLU, a bilinear MLP and a SwiGLU" loading="lazy" decoding="async" fetchpriority="auto" width="220" height="130" class="inline-block w-full max-w-md"><figcaption><p>An swiGLU has a gate part which “selects” which parts of the other side should be kept.
Bilinear layers simply omit this gate and keep continuous interactions instead.</p></figcaption></figure>
<p>As foreshadowed, the linearity of each branch is helpful for interpretability. In this context, techniques from linear algebra, such as SVD, are now actually meaningful as they do not ignore the activation function. However, that’s not all. Each output of a bilinear layer is equivalent to a sum of (weighted) pairwise input feature interactions. Note that this is impossible in a layer with a ReLU (or other activation functions); no simple formula describes how features interact without considering the inputs.</p>
<h3 id="decomposing-the-weights">Decomposing The Weights</h3>
<p>Because of the bilinear layer’s appealing properties, it becomes possible to trace from output to input, which was previously only possible using gradient-based methods. This allows us to decompose the model into the most important components for any desired output direction. While we won’t go into details, these are the main ideas behind the decomposition.</p>
<ol>
<li>
<p>The feature interactions of a bilinear layer output can be written as <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="bold">x</mtext><mi>A</mi><mtext mathvariant="bold">x</mtext></mrow><annotation encoding="application/x-tex">\textbf{x}A\textbf{x}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord text"><span class="mord textbf">x</span></span><span class="mord mathnormal">A</span><span class="mord text"><span class="mord textbf">x</span></span></span></span></span> where we call <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">A</span></span></span></span> the interaction matrix. Each entry in the matrix is the weight of how strongly the feature at the row and column should interact. Due to this structure, it is a symmetric matrix. While this is nice, this interaction matrix is complex to visualize for any non-trivial problem (especially in spatially dependent modalities, such as images).</p>
</li>
<li>
<p>We can leverage this symmetric property and perform an eigendecomposition on these matrices. This operation decomposes the matrix into <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mo>∑</mo><mi>i</mi></msub><msub><mi>λ</mi><mi>i</mi></msub><msub><mi>v</mi><mi>i</mi></msub><mo>⊗</mo><msub><mi>v</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\sum_i \lambda_i v_i \otimes v_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0497em;vertical-align:-0.2997em"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.162em"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2997em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">λ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⊗</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> (where <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>⊗</mo></mrow><annotation encoding="application/x-tex">\otimes</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em"></span><span class="mord">⊗</span></span></span></span> is an outer product) where values of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">λ</span></span></span></span> decrease. In other words, this finds which vectors best describe the interaction matrix. We can then easily visualize these vectors.</p>
</li>
<li>
<p>Since a sum of symmetric matrices is still symmetric, we can decompose any output direction, not just single outputs. For instance, we can take the difference between two features by subtracting the interaction matrices.</p>
</li>
</ol>
<p>The following images depict the most important eigenvector for a set of digits in a single-layer MNIST model.</p>
<div class="grid grid-cols-4 gap-4"><img src="/_astro/digit_0.D7-nP5Zw_ZSRcfe.svg" alt="digit 0" loading="lazy" decoding="async" fetchpriority="auto" width="374" height="374"><img src="/_astro/digit_1.DdlsJAQ2_ZSRcfe.svg" alt="digit 1" loading="lazy" decoding="async" fetchpriority="auto" width="374" height="374"><img src="/_astro/digit_5.mfkvcQm-_ZSRcfe.svg" alt="digit 5" loading="lazy" decoding="async" fetchpriority="auto" width="374" height="374"><img src="/_astro/digit_6.DPabdhzi_ZSRcfe.svg" alt="digit 6" loading="lazy" decoding="async" fetchpriority="auto" width="374" height="374"></div>
<p>This shows that exploiting these properties of the bilinear layer can yield very interpretable structure from the weights alone. We use this towards multiple ends in the paper.</p>
<ul>
<li>Reverse-engineering an algorithmic task in an image classification model.</li>
<li>Finding adversarial examples from the weights.</li>
<li>Qualitatively studying the impact of regularization/augmentation.</li>
<li>Post-hoc explanations of (mis)classification.</li>
</ul>
<h3 id="language-models">Language Models</h3>
<p>Each output of a bilinear layer is described by weighted pairwise interactions between their input
features. Beyond tracing between the actual output classes and the input, we can use this technique to trace between latent features in large models.
We use sparse dictionary learning, which can be seen as unsupervised probing, to create a semantically meaningful set of features (called a dictionary) around a hidden bilinear layer.
While such dictionary-based approaches have seen success in the past, they only indicate what features a model uses, not how they are used.
Using our method, we can understand the interactions between such dictionaries, based on the weights. This has several advantages.</p>
<ul>
<li>By finding the common structure between dictionary elements through eigenvectors, we partially resolve phenomena such as feature splitting.</li>
<li>Understanding how a feature is formed gives more insight into what a feature actually does.</li>
</ul>
<p>We apply this approach to a 6-layer language model, trained on children’s stories.
These stories provide a more controlled environment compared to general datasets.</p>
<figure class="text-center"><img src="/_astro/subspace.D63T13-K_Z2tnFJW.svg" alt="feature subspace" loading="lazy" decoding="async" fetchpriority="auto" width="800" height="600" class="inline-block"></figure>
<p>We highlight one particular circuit that we extracted, which performs a form of sentiment negation.
This circuit performs an AND operation between sentiment features (firing broadly on positive and negative segments) and negation features (which fires on tokens which as not and isn’t).
It then flips this sentiment in the output direction.</p>
<h3 id="future-work">Future Work</h3>
<p>Due to the generality and novelty of this technique, we haven’t yet explored many possibilities for interpretability.
The current plan is to focus on continuing the work on language models and seeing which kinds of mechanisms we’re able to detect.</p> </div> <div class="pt-6"></div> <div class="card bg-gray-100 shadow-md hidden md:block"> <div class="flex items-center justify-between px-4 py-2 bg-gray-200"> <h3 class="font-semibold text-gray-900">How to cite</h3> <button class="btn btn-ghost btn-sm" onclick="navigator.clipboard.writeText(`@misc{pearce2025bilinearmlpsenableweightbased,
      title={Bilinear MLPs enable weight-based mechanistic interpretability}, 
      author={Michael T. Pearce and Thomas Dooms and Alice Rigg and Jose M. Oramas and Lee Sharkey},
      year={2025},
      eprint={2410.08417},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.08417}, 
}
`)" aria-label="Copy citation"> <i class="fa-solid fa-copy text-base"></i> </button> </div> <pre class="p-4 text-xs overflow-x-auto whitespace-pre-wrap break-words">@misc{pearce2025bilinearmlpsenableweightbased,
      title={Bilinear MLPs enable weight-based mechanistic interpretability}, 
      author={Michael T. Pearce and Thomas Dooms and Alice Rigg and Jose M. Oramas and Lee Sharkey},
      year={2025},
      eprint={2410.08417},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.08417}, 
}
</pre> </div>  <div class="pb-2" data-astro-cid-sckkx6r4></div> </main> </body></html>